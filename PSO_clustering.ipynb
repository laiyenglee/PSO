{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSO_clustering",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMo5UQSeB8DDdgTDcDlgg7x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laiyenglee/PSO/blob/main/PSO_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KEWWa3uyper"
      },
      "outputs": [],
      "source": [
        "import numpy as np                               #the module numpy is imported as np and it is used to perform a wide variety of mathematical operations on arrays.\n",
        "import matplotlib.pyplot as plt                  #the module matplotlib.pyplot is imported as plt and it is is a plotting library used for 2D graphics.\n",
        "import pandas as pd                              #the module pandas is imported as pd and it is used for data analysis.\n",
        "from typing import Tuple                         #Tuple is imported from typing module and it is used to specify a specific number of expected elements and the type of each position.\n",
        "#It turns on \"inline plotting‚Äù.\n",
        "%matplotlib inline                               "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "#sklearn.metrics is a module in scikit-learn that used to access the quality of predictions and compute the performance metrics like f1 score, recall and precision.\n",
        "from sklearn.cluster import KMeans\n",
        "#Implementation of k-means clustering from the popular machine learning package scikit-learn.\n",
        "from sklearn.preprocessing import normalize\n",
        "#used for scaling input data set"
      ],
      "metadata": {
        "id": "dYjuLnXzJoZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#mount Google Drive on runtime\n",
        "#Read data from google drive"
      ],
      "metadata": {
        "id": "ikv_SiXc16bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('gdrive/My Drive/Bloodtype.txt', sep='\\t', header=None)\n",
        "data.head()\n",
        "#read data in csv"
      ],
      "metadata": {
        "id": "ZRZiJBPQy0ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Particle:                                                                               #class Particle is defined.\n",
        "    def __init__(self, n_clusters, data, use_kmeans=True, w=0.72, c1=2.05, c2=2.05):          #Initialize the parameters\n",
        "        self.n_clusters = n_clusters                                                          #KMeans clustering\n",
        "        if use_kmeans:\n",
        "            k_means = KMeans(n_clusters=self.n_clusters)                                      #create kmeans cluster with n_cluster\n",
        "            k_means.fit(data)                                                                 #pass the data to cluster\n",
        "            self.centroids_position = k_means.cluster_centers_                                #centroids position is the location representing the center of the cluster\n",
        "        else:\n",
        "            self.centroids_position = data[np.random.choice(list(range(len(data))), self.n_clusters)]\n",
        "\n",
        "        # each cluster has a centroid which is the center of the cluster\n",
        "        # assign k random data to k centroids\n",
        "        self.pbest_value = np.inf\n",
        "        # personal best position for all the centroids so far\n",
        "        self.pbest_position = self.centroids_position.copy()\n",
        "        self.velocity = np.zeros_like(self.centroids_position)\n",
        "        self.pbest_clustering = None                                                          # best data clustering so far\n",
        "        # pso parameters w,c1,c2\n",
        "        self.w = w\n",
        "        self.c1 = c1\n",
        "        self.c2 = c2\n",
        "\n",
        "    def update_pbest(self, data: np.ndarray):\n",
        "        \"\"\"\n",
        "        Updates the personal best score based on the fitness function of particles\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # finding out which data points belongs to which cluster\n",
        "        # find the distance between the data points and the centroids.\n",
        "        distances = self.get_distances(data=data)\n",
        "        # the minimum distance between a data point and a centroid indicates that, the point belongs to that cluster.\n",
        "        clusters = np.argmin(distances, axis=0)  # shape: (len(data),)\n",
        "        clusters_ids = np.unique(clusters)\n",
        "\n",
        "        # When the algorithm generates less than n clusters\n",
        "        # find the cluster id that is missed and generate the centroid position with a random data point\n",
        "        while len(clusters_ids) != self.n_clusters:\n",
        "            deleted_clusters = np.where(np.isin(np.arange(self.n_clusters), clusters_ids) == False)[0]\n",
        "            self.centroids_position[deleted_clusters] = data[np.random.choice(list(range(len(data))), len(deleted_clusters))]\n",
        "            distances = self.get_distances(data=data)\n",
        "            clusters = np.argmin(distances, axis=0)\n",
        "            clusters_ids = np.unique(clusters)\n",
        "\n",
        "        #new_value is determined according to the fitness function of each particle\n",
        "        new_value = self.fitness_function(clusters=clusters, distances=distances)\n",
        "        if new_value < self.pbest_value:\n",
        "            self.pbest_value = new_value\n",
        "            self.pbest_position = self.centroids_position.copy()\n",
        "            self.pbest_clustering = clusters.copy()\n",
        "\n",
        "    def update_velocity(self, gbest_position: np.ndarray):\n",
        "        \"\"\"\n",
        "        Updates the new velocity based on the current velocity, personal best position and the global best position.\n",
        "        :parameter gbest_position: vector of best centroid positions among all particles so far\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        #The new velocity is updated based on the equation (4) in this paper.\n",
        "        self.velocity = self.w * self.velocity + \\\n",
        "                        self.c1 * np.random.random() * (self.pbest_position - self.centroids_position) + \\\n",
        "                        self.c2 * np.random.random() * (gbest_position - self.centroids_position)\n",
        "\n",
        "    def move_centroids(self, gbest_position):\n",
        "        \"\"\" \n",
        "        Moves the centroids position based on the global best position among the particles.\n",
        "        \"\"\"\n",
        "        #The new position is updated based on the equation (3) in this paper.\n",
        "        self.update_velocity(gbest_position= gbest_position)\n",
        "        new_position = self.centroids_position + self.velocity\n",
        "        self.centroids_position = new_position.copy()\n",
        "\n",
        "    def get_distances(self, data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculates the Euclidean distance between data and centroids\n",
        "        :parameters data:\n",
        "        :return: distances: a numpy array of distances (len(centroids) x len(data))\n",
        "        \"\"\"\n",
        "        distances = []\n",
        "        for centroid in self.centroids_position:\n",
        "            #The euclidean distance is the square root of sum of absolute squares.\n",
        "            d = np.linalg.norm(data - centroid, axis=1)\n",
        "            distances.append(d)\n",
        "        distances = np.array(distances)\n",
        "        return distances\n",
        "\n",
        "    def fitness_function(self, clusters: np.ndarray, distances: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Calculates the fitness function, where\n",
        "        i is the index of particle\n",
        "        j is the index of clusters in the particle i\n",
        "        p is the vector of the input data indices belonging the cluster[ij]\n",
        "        z[p] is the vector of the input data belonging the cluster[ij]\n",
        "        d is a vector of distances between z(p) and centroid j\n",
        "        :parameter clusters:\n",
        "        :parameter distances:\n",
        "        :return: J:\n",
        "        \"\"\"\n",
        "        J = 0.0\n",
        "        for i in range(self.n_clusters):\n",
        "            p = np.where(clusters == i)[0]\n",
        "            if len(p):\n",
        "                d = sum(distances[i][p])\n",
        "                d /= len(p)\n",
        "                J += d\n",
        "        J /= self.n_clusters\n",
        "        return J"
      ],
      "metadata": {
        "id": "tHqKhKA_6H_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class PSOClustering is defined.\n",
        "class PSOClustering:\n",
        "    def __init__(self, n_clusters: int, n_particles: int, data: np.ndarray, hybrid=True, w=0.72, c1=2.05, c2=2.05):\n",
        "        \"\"\"\n",
        "        Initializes the parameters of the swarm.\n",
        "        :param n_clusters: number of clusters\n",
        "        :param n_particles: number of particles\n",
        "        :param data: ( number_of_points x dimensions)\n",
        "        :param hybrid: bool : whether or not use kmeans as seeding\n",
        "        :param w: w\n",
        "        :param c1: c1\n",
        "        :param c2: c2\n",
        "        \"\"\"\n",
        "        self.n_clusters = n_clusters\n",
        "        self.n_particles = n_particles\n",
        "        self.data = data\n",
        "\n",
        "        self.particles = []\n",
        "        # for storing global best\n",
        "        self.gbest_position = None\n",
        "        self.gbest_value = np.inf\n",
        "        # global best data clustering so far\n",
        "        # for each data point will contain the cluster number\n",
        "        self.gbest_clustering = None\n",
        "\n",
        "        # hybrid PSO algorithm\n",
        "        self.generate_particles(hybrid, w, c1, c2)\n",
        "\n",
        "    def print_initial(self, iteration, plot):\n",
        "        \"\"\"\n",
        "        Prints the message of the plotting swarm like number of particles, number of clusters, maximum iterations at the initial.\n",
        "        \"\"\"\n",
        "        print('Initialing the swarm with', self.n_particles, 'PARTICLES, ', self.n_clusters, 'CLUSTERS with', iteration,\n",
        "              'MAXIMUM ITERATIONS and with PLOT =', plot )\n",
        "        print('Data=', self.data.shape[0], 'points in', self.data.shape[1], 'dimensions')\n",
        "\n",
        "    def generate_particles(self, hybrid: bool, w: float , c1: float, c2: float ):\n",
        "        \"\"\"\n",
        "        Generates particles with k clusters and t-dimensional points\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        for i in range(self.n_particles):\n",
        "            particle = Particle(n_clusters=self.n_clusters, data=self.data, use_kmeans=hybrid, w=w, c1=c1, c2=c2)\n",
        "            self.particles.append(particle)\n",
        "\n",
        "    def update_gbest(self, particle):\n",
        "        \"\"\"\n",
        "        Updates the global best position of the particle according to the fitness function of particle.\n",
        "        \"\"\"\n",
        "        if particle.pbest_value < self.gbest_value:\n",
        "            self.gbest_value = particle.pbest_value\n",
        "            self.gbest_position = particle.pbest_position.copy()\n",
        "            self.gbest_clustering = particle.pbest_clustering.copy()\n",
        "\n",
        "    def start(self, iteration=2000, plot=False) -> Tuple[np.ndarray, float]:\n",
        "        \"\"\"\n",
        "        Start to plot the clusters of data\n",
        "        :param plot: = True will plot the global best data clusters\n",
        "        :param iteration: number of maximum iteration\n",
        "        :return: (best cluster, best fitness value)\n",
        "        \"\"\"\n",
        "        self.print_initial(iteration, plot)\n",
        "        progress = []\n",
        "        # Iterate until the max iteration\n",
        "        for i in range(iteration):\n",
        "            if i % 400 == 0:                                                 #plot the global best data clusters for every 400 iterations.\n",
        "                clusters = self.gbest_clustering\n",
        "                print('iteration', i, 'GBest =', self.gbest_value)           #print the iteration number and gbest value for every plotting\n",
        "                print('best clusters so far = ', clusters)\n",
        "\n",
        "                #determine the style and colour of plotting\n",
        "                if plot:\n",
        "                    centroids = self.gbest_position\n",
        "                    if clusters is not None:\n",
        "                        plt.scatter(self.data[:, 0], self.data[:, 1], c=clusters, cmap='plasma')\n",
        "                        plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=50, alpha=0.5)\n",
        "                        plt.show()\n",
        "                    else:  # if there is no clusters yet ( iteration = 0 ) plot the data with no clusters\n",
        "                        plt.scatter(self.data[:, 0], self.data[:, 1])\n",
        "                        plt.show()\n",
        "\n",
        "            #Update the personal best and global best of particle.\n",
        "            for particle in self.particles:\n",
        "                particle.update_pbest(data=self.data)\n",
        "                self.update_gbest(particle=particle)\n",
        "\n",
        "            for particle in self.particles:\n",
        "                particle.move_centroids(gbest_position=self.gbest_position)\n",
        "            progress.append([self.gbest_position, self.gbest_clustering, self.gbest_value])\n",
        "\n",
        "        #Print 'Finished' when the process of PSO clustering is completed.   \n",
        "        print('Finished!')\n",
        "        return self.gbest_clustering, self.gbest_value"
      ],
      "metadata": {
        "id": "-IoqzICO7DU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the global best data clusters\n",
        "plot = True\n",
        "# Read data from the google drive.\n",
        "data = pd.read_csv('gdrive/My Drive/Bloodtype.txt', sep='\\t',error_bad_lines=False, header=None)\n",
        "# cluster the data based on the values of column[7] of the data.\n",
        "clusters = data[7].values\n",
        "#visualize the data by dropping the column[7] of the data.\n",
        "data = data.drop([7], axis=1)"
      ],
      "metadata": {
        "id": "uYU_tv8i8unH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot by using 2 points of data\n",
        "if plot:\n",
        "    data = data[[0, 1]]\n",
        "# convert to numpy 2d array\n",
        "data = data.values\n",
        "# hybrid PSO algorithm is used in 50 particles to cluster the data in 4 clusters.\n",
        "pso = PSOClustering(n_clusters=4, n_particles=50, data=data, hybrid=True)\n",
        "# plot the clusters of data for maximum iteration of 2000.\n",
        "pso.start(iteration=2000, plot=plot)\n"
      ],
      "metadata": {
        "id": "CMSC5BzHAKbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assign 4 clusters\n",
        "pso_kmeans= KMeans(n_clusters=4)\n",
        "#pass the data to cluster\n",
        "pso_kmeans.fit(data)\n",
        "#apply a trained model to data\n",
        "y_pred=pso_kmeans.predict(data) \n",
        "#identify the maximum value for every row\n",
        "data=np.argmax(data, axis=1)"
      ],
      "metadata": {
        "id": "K09jRGaPD0Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the f1 score for the data in multiclass or multilabel \n",
        "# Calculate metrics for each label, and find their unweighted mean.\n",
        "print('F1 score:',f1_score(data,y_pred ,average='macro'))"
      ],
      "metadata": {
        "id": "QZVi63BVM3qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the presicion for the data in multiclass or multilabel\n",
        "# Calculate metrics for each label, and find their unweighted mean.\n",
        "print('Precision:',precision_score(data,y_pred,average='macro'))"
      ],
      "metadata": {
        "id": "7OBONQWGMxOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the recall or the data in multiclass or multilabel\n",
        "# Calculate metrics for each label, and find their unweighted mean.\n",
        "print('Recall:', recall_score(data,y_pred ,average='macro'))"
      ],
      "metadata": {
        "id": "2qqQM1jdMzYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For showing the actual clusters\n",
        "mapping = {'A': 0, 'B': 1, 'AB': 2, 'O':3}\n",
        "clusters = np.array([mapping[x] for x in clusters])\n",
        "print('Actual classes = ', clusters)"
      ],
      "metadata": {
        "id": "wKj3h4uwARyo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}